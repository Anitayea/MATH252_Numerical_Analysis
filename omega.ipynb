{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "omega.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4DL+zGgCZrtUYkrRjYWFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anitayea/Numerical_Analysis/blob/main/omega.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1\n",
        "\n",
        "\n",
        "$\\omega A = (D-\\omega L) - [\\omega U + (1-\\omega) D]$\n",
        "\n",
        "\n",
        "When A is positive definite, the interations converges for some guess, $\\phi$, with $0 < \\omega <2$. Then, Matrix G is given by:\n",
        "\n",
        "$\\phi^{n+1} = (D-\\omega L)^{-1} * (\\omega b +[\\omega U + (1-\\omega)D] \\phi^n)$\n",
        "\n",
        "\\\\\n",
        "Turning it into a eigenvalue problem we have:\n",
        "\n",
        "$|G-\\lambda I| = |(I-\\omega D^-1L)^{-1} D^{-1} [(\\lambda+\\omega -1)D - \\lambda \\omega L - \\omega U]|=0$\n",
        "\n",
        "\\\\\n",
        "The determinant of a lower triangular matrix, the first group, is one, and $D^{-1}$ can be cancelled, so we have: \n",
        "$|(\\lambda+\\omega -1)D - \\lambda \\omega L - \\omega U| =0$\n",
        "\n",
        "\\\\\n",
        "The eigenvalues of SOR matrix is closely related tyo the eigenvalues of Jacobi matrix, so by:\n",
        "\n",
        "$(\\lambda + \\omega -1)^2 - \\lambda \\omega^2 \\lambda_{Jac}^2 =0$\n",
        "\n",
        "\\\\\n",
        "Therefore, to maximize the rate of convergence, we minimize the spectral radius:\n",
        "\n",
        "$\\omega = 2/(1+\\sqrt{1-\\rho^2 (G_{Jac})})$\n"
      ],
      "metadata": {
        "id": "1MQWQjVjozdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2\n",
        "\n",
        "We calculate omega using the convergence rate, so the spectral radius for SOR to maximize the efficiency. \n",
        "\n",
        "\n",
        "We need to know that:\n",
        "\n",
        "1. $ \\omega \\in (0,2)$\n",
        "2. For Jacobi's iteration matrix, $C_{Jac} := I - D^-1 A$ has only one real eigenvalue.\n",
        "3. Jacobi's method converges: $\\mu := \\rho (C_{Jac}<1$\n",
        "4. Matrix decomposition $A = D+L+U$ satisfies:\n",
        "$det(\\lambda D + zL + 1/z U) = det(\\lambda D + L + U)$ for any $ z \\in C*, \\lambda \\in C$\n",
        "5. The last one holds for tridiagonal matrices: \n",
        "$ Z(\\lambda D+L+U)Z^-1 = \\lambda D+zL+1/zU$ \n",
        "\n",
        "for diagonal Z with entries \n",
        "$Z_ii = z^{i-1}$ and $det(\\lambda D + L + U)=det(Z(\\lambda D + L + U)Z^-1)$\n",
        "\n",
        "Therefore, the convergence rate is:\n",
        "\n",
        "$\\rho(C_\\omega)=$\n",
        "\\begin{cases}{\\frac {1}{4}}\\left(\\omega \\mu +{\\sqrt {\\omega ^{2}\\mu ^{2}-4(\\omega -1)}}\\right)^{2}\\,,&0<\\omega \\leq \\omega _{\\text{opt}}\\\\\\omega -1\\,,&\\omega _{\\text{opt}}<\\omega <2\\end{cases}\n",
        "\n",
        "\n",
        "Therefore, the optimal relaxation parameter is:\n",
        "\n",
        "$\\omega_opt := 1+(\\mu /(1+\\sqrt{1-\\mu^2})2)^2$"
      ],
      "metadata": {
        "id": "hvNb9Pltcnd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ps. spectral radius r is the absolute value of the maximum of the eigenvalues of $I-D/A$, and to find the omega we need to take the $\\sqrt{(1-r^2)}$ of it. Cases where $\\omega = 1$ is simply Gauss-Seidal, which comes from Jacobi. We uses the spectral radius from Jacobi as the relationship between $\\omega$ of iterative methods and $\\rho$ is shown to be linear and after the optimal $\\omega$. That is, when the optimal value is reached, $\\mu$ does not change $\\rho$ anymore. "
      ],
      "metadata": {
        "id": "HnOvHOF6iq65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is mostly by research:)"
      ],
      "metadata": {
        "id": "jiq61t13obOo"
      }
    }
  ]
}