{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NAmid.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPNOkxoO15b00FIWxC04BOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anitayea/Numerical_Analysis/blob/main/NAmid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of examinable proofs\n",
        "\n",
        "\n",
        "This list collects the examinable results for this course. It will grow and may be modified as the course progresses. You don’t have to remember the statements of the results, but should be able to prove them given the statements.\n",
        "\n",
        "\n",
        "#Chapter 2\n",
        "\n",
        "• Proposition 2.1 (upper bound on the condition number with respect to perturbations of the right-hand side);\n",
        "\n",
        "• Proposition 2.2 (upper bound the on condition number with respect to perturbations of the matrix), if you are given the preceding Lemma 2.3;\n",
        "\n",
        "• Lemma 2.5 (explicit expression of the matrix L given the parameters of the Gaussian transformations).\n",
        "\n",
        "• Proposition A.8, in the case where A is diagonalizable (equivalence between ρ(A) < 1 and the convergence kA kk → 0 as k → ∞).\n",
        "\n",
        "• Proposition 2.10 when given Gelfand’s formula for granted (convergence of the general splitting method).\n",
        "\n",
        "• Derivation of the optimal ω in Richardson’s method.\n",
        "\n",
        "• Proposition 2.11 (convergence of Jacobi’s method in the case of a strictly diagonally dominant matrix).\n",
        "\n",
        "• Proposition 2.12 and Corollary 2.13 (convergence of the relaxation method for Hermitian and positive definite A).\n",
        "\n",
        "• Theorem 2.17 given the Kantorovich inequality (convergence of the steepest descent method).\n",
        "\n",
        "\n",
        "#Chapter 3\n",
        "• Theorem 3.2 (global exponential convergence of the fixed point iteration).\n",
        "\n",
        "• Proposition 3.4 (local exponential convergence of the fixed point iteration under a local Lipschitz condition).\n",
        "\n",
        "• Proposition 3.5 (local exponential convergence given bound on the Jacobian matrix).\n",
        "\n",
        "iv\n",
        "\n",
        "• Proposition 3.6 (superlinear convergence of fixed point iteration when the Jacobian is zeroat the fixed point).\n",
        "\n",
        "Chapter 4\n",
        "• Proposition 4.1 (Convergence of the power iteration).\n",
        "v"
      ],
      "metadata": {
        "id": "lqcaekPlLRpk"
      }
    }
  ]
}